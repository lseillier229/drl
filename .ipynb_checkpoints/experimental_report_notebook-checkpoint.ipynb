{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapport Exp√©rimental - Algorithmes d'Apprentissage par Renforcement\n",
    "\n",
    "**Projet**: (Deep) Reinforcement Learning P1  \n",
    "**Ann√©e**: 2024-2025  \n",
    "**Enseignant**: Nicolas VIDAL\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "Ce rapport pr√©sente une √©valuation comparative compl√®te des algorithmes d'apprentissage par renforcement classiques :\n",
    "\n",
    "1. **Dynamic Programming** : Policy Iteration, Value Iteration\n",
    "2. **Monte Carlo** : ES, On-policy, Off-policy\n",
    "3. **Temporal Difference** : SARSA, Q-Learning, Expected SARSA\n",
    "4. **Planning** : Dyna-Q, Dyna-Q+\n",
    "\n",
    "### Questions de recherche\n",
    "\n",
    "- Quel algorithme est le plus performant sur quel environnement ?\n",
    "- Comment les hyperparam√®tres affectent-ils les performances ?\n",
    "- Quels sont les compromis vitesse/qualit√© pour chaque m√©thode ?\n",
    "- Quelles recommandations pratiques en d√©coulent ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Imports du projet\n",
    "from experiments import ExperimentRunner, generate_report\n",
    "from hyperparameter_studies import main as run_hyperparameter_studies\n",
    "from envs import LineWorld, GridWorld, RPS, MontyHall1, MontyHall2\n",
    "from algos import *\n",
    "\n",
    "print(\"‚úÖ Setup termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pr√©sentation des Environnements\n",
    "\n",
    "### 1.1 LineWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration LineWorld\n",
    "env = LineWorld()\n",
    "print(f\"üìè LineWorld : {env.num_states()} √©tats, {env.num_actions()} actions\")\n",
    "print(f\"√âtat initial : {env.state()}\")\n",
    "print(f\"Score initial : {env.score()}\")\n",
    "print(\"\\nTest de quelques actions :\")\n",
    "\n",
    "# Simulation d'un √©pisode\n",
    "env.reset()\n",
    "actions = [0, 0, 1, 1, 1]  # gauche, gauche, droite, droite, droite\n",
    "for i, action in enumerate(actions):\n",
    "    if not env.is_game_over():\n",
    "        prev_state = env.state()\n",
    "        prev_score = env.score()\n",
    "        env.step(action)\n",
    "        print(f\"  √âtape {i+1}: action={action}, √©tat={prev_state}‚Üí{env.state()}, score={prev_score}‚Üí{env.score()}\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"\\nüèÅ √âpisode termin√©. Score final : {env.score()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration GridWorld\n",
    "env = GridWorld()\n",
    "print(f\"üèóÔ∏è GridWorld : {env.num_states()} √©tats, {env.num_actions()} actions\")\n",
    "print(f\"Actions : 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\")\n",
    "print(f\"\\n√âtat initial :\")\n",
    "env.render()\n",
    "\n",
    "# Test d'une s√©quence d'actions\n",
    "actions = [1, 1, 1, 1, 2, 2, 2, 2]  # droite√ó4, bas√ó4\n",
    "print(\"S√©quence d'actions : RIGHT√ó4, DOWN√ó4\")\n",
    "for action in actions:\n",
    "    if not env.is_game_over():\n",
    "        env.step(action)\n",
    "        \n",
    "print(f\"\\n√âtat final :\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Rock-Paper-Scissors (RPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration RPS\n",
    "env = RPS()\n",
    "print(f\"‚úÇÔ∏è Rock-Paper-Scissors : {env.num_states()} √©tats, {env.num_actions()} actions\")\n",
    "print(f\"Actions : 0=ROCK, 1=PAPER, 2=SCISSORS\")\n",
    "print(f\"\\nR√®gles :\")\n",
    "print(f\"- Manche 1 : adversaire joue al√©atoirement\")\n",
    "print(f\"- Manche 2 : adversaire copie votre coup de la manche 1\")\n",
    "print(f\"- Score = somme des r√©sultats des 2 manches\")\n",
    "\n",
    "# Simulation d'un √©pisode\n",
    "env.reset()\n",
    "print(f\"\\n√âtat initial : {env.state()}\")\n",
    "env.render()\n",
    "\n",
    "# Jouer PAPER puis ROCK\n",
    "print(\"\\nüéÆ Action 1 : PAPER\")\n",
    "env.step(1)  # PAPER\n",
    "env.render()\n",
    "\n",
    "print(\"\\nüéÆ Action 2 : ROCK\")\n",
    "env.step(0)  # ROCK  \n",
    "env.render()\n",
    "\n",
    "print(f\"\\nüèÅ Score final : {env.score()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Monty Hall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration Monty Hall 1 (3 portes)\n",
    "env = MontyHall1()\n",
    "print(f\"üö™ Monty Hall 1 : {env.num_states()} √©tats, {env.num_actions()} actions\")\n",
    "print(f\"Actions : 0=Porte A, 1=Porte B, 2=Porte C\")\n",
    "\n",
    "# Simulation de la strat√©gie \"toujours changer\"\n",
    "total_wins = 0\n",
    "num_simulations = 1000\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "    env.reset()\n",
    "    \n",
    "    # Premi√®re action : choisir porte 0\n",
    "    env.step(0)\n",
    "    \n",
    "    # Deuxi√®me action : changer (choisir une porte diff√©rente de 0)\n",
    "    # L'environnement nous indique quelles portes sont disponibles via l'√©tat\n",
    "    available_doors = [1, 2]  # Simplifi√© pour la d√©mo\n",
    "    env.step(available_doors[0])\n",
    "    \n",
    "    if env.score() > 0:\n",
    "        total_wins += 1\n",
    "\n",
    "win_rate = total_wins / num_simulations\n",
    "print(f\"\\nüìä Strat√©gie 'toujours changer' : {win_rate:.1%} de victoires\")\n",
    "print(f\"üìö Th√©orie : ~66.7% attendu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exp√©rimentation Principale\n",
    "\n",
    "### 2.1 Comparaison tous algorithmes √ó tous environnements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancement de l'exp√©rimentation principale\n",
    "print(\"üöÄ Lancement de l'exp√©rimentation compl√®te...\")\n",
    "print(\"‚è±Ô∏è Cela peut prendre plusieurs minutes...\")\n",
    "\n",
    "runner = ExperimentRunner(\"results\")\n",
    "\n",
    "# Environnements √† tester\n",
    "environments = [\"lineworld\", \"gridworld\", \"rps\", \"montyhall1\"]\n",
    "# Note: montyhall2 omis pour des raisons de temps d'ex√©cution\n",
    "\n",
    "# Algorithmes √† tester  \n",
    "algorithms = [\n",
    "    \"policy_iteration\", \"value_iteration\",           # DP\n",
    "    \"mc_es\", \"mc_on_policy\", \"mc_off_policy\",       # MC\n",
    "    \"sarsa\", \"q_learning\", \"expected_sarsa\",        # TD\n",
    "    \"dyna_q\"                                         # Planning\n",
    "]\n",
    "\n",
    "# Ex√©cution\n",
    "results_df, detailed_results = runner.run_full_comparison(environments, algorithms)\n",
    "\n",
    "print(\"‚úÖ Exp√©rimentation termin√©e !\")\n",
    "print(f\"üìä {len(results_df)} exp√©riences r√©alis√©es\")\n",
    "\n",
    "# Aper√ßu des r√©sultats\n",
    "print(\"\\nüìã Aper√ßu des r√©sultats :\")\n",
    "display(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analyse des Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques g√©n√©rales\n",
    "print(\"üìà STATISTIQUES G√âN√âRALES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Performance moyenne par algorithme\n",
    "algo_stats = results_df.groupby(\"Algorithm\")[\"Final Score\"].agg([\"mean\", \"std\", \"count\"]).round(3)\n",
    "algo_stats = algo_stats.sort_values(\"mean\", ascending=False)\n",
    "print(\"\\nüèÜ Classement des algorithmes (score moyen) :\")\n",
    "display(algo_stats)\n",
    "\n",
    "# Performance par environnement\n",
    "env_stats = results_df.groupby(\"Environment\")[\"Final Score\"].agg([\"mean\", \"std\", \"min\", \"max\"]).round(3)\n",
    "print(\"\\nüéØ Performance par environnement :\")\n",
    "display(env_stats)\n",
    "\n",
    "# Vitesse de convergence\n",
    "conv_stats = results_df.groupby(\"Algorithm\")[\"Convergence Episode\"].agg([\"mean\", \"std\"]).round(1)\n",
    "conv_stats = conv_stats.sort_values(\"mean\")\n",
    "print(\"\\n‚ö° Vitesse de convergence (√©pisodes) :\")\n",
    "display(conv_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations des performances\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Heatmap des performances\n",
    "ax = axes[0, 0]\n",
    "pivot_scores = results_df.pivot(index=\"Algorithm\", columns=\"Environment\", values=\"Final Score\")\n",
    "sns.heatmap(pivot_scores, annot=True, fmt=\".3f\", cmap=\"viridis\", ax=ax)\n",
    "ax.set_title(\"Performance des Algorithmes par Environnement\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Boxplot des performances par algorithme\n",
    "ax = axes[0, 1]\n",
    "results_df.boxplot(column=\"Final Score\", by=\"Algorithm\", ax=ax, rot=45)\n",
    "ax.set_title(\"Distribution des Performances par Algorithme\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Algorithme\")\n",
    "ax.set_ylabel(\"Score Final\")\n",
    "\n",
    "# 3. Temps de convergence vs Performance\n",
    "ax = axes[1, 0]\n",
    "for algo in results_df[\"Algorithm\"].unique():\n",
    "    algo_data = results_df[results_df[\"Algorithm\"] == algo]\n",
    "    ax.scatter(algo_data[\"Convergence Episode\"], algo_data[\"Final Score\"], \n",
    "              label=algo, alpha=0.7, s=60)\n",
    "ax.set_xlabel(\"√âpisodes jusqu'√† Convergence\")\n",
    "ax.set_ylabel(\"Score Final\")\n",
    "ax.set_title(\"Compromis Vitesse vs Performance\", fontsize=14, fontweight='bold')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Temps d'ex√©cution par algorithme\n",
    "ax = axes[1, 1]\n",
    "time_stats = results_df.groupby(\"Algorithm\")[\"Training Time (s)\"].mean().sort_values()\n",
    "time_stats.plot(kind='bar', ax=ax, color='coral')\n",
    "ax.set_title(\"Temps d'Ex√©cution Moyen par Algorithme\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Algorithme\")\n",
    "ax.set_ylabel(\"Temps (secondes)\")\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analyse par Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse d√©taill√©e par environnement\n",
    "for env_name in environments:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéØ ANALYSE : {env_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    env_data = results_df[results_df[\"Environment\"] == env_name]\n",
    "    \n",
    "    if len(env_data) == 0:\n",
    "        print(\"‚ùå Aucune donn√©e disponible\")\n",
    "        continue\n",
    "    \n",
    "    # Classement des algorithmes\n",
    "    ranking = env_data.sort_values(\"Final Score\", ascending=False)\n",
    "    print(\"\\nüèÜ Classement des algorithmes :\")\n",
    "    for i, (_, row) in enumerate(ranking.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['Algorithm']:15} | Score: {row['Final Score']:6.3f} | \"\n",
    "              f\"Convergence: {row['Convergence Episode']:4.0f} √©pisodes | \"\n",
    "              f\"Temps: {row['Training Time (s)']:5.2f}s\")\n",
    "    \n",
    "    # Recommandation\n",
    "    best_algo = ranking.iloc[0]\n",
    "    print(f\"\\n‚úÖ RECOMMANDATION : {best_algo['Algorithm']}\")\n",
    "    print(f\"   Justification : Score optimal ({best_algo['Final Score']:.3f}) avec \"\n",
    "          f\"convergence en {best_algo['Convergence Episode']:.0f} √©pisodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. √âtudes des Hyperparam√®tres\n",
    "\n",
    "### 3.1 Impact du Learning Rate (Œ±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtude simplifi√©e du learning rate\n",
    "print(\"üî¨ √âtude de l'impact du learning rate...\")\n",
    "\n",
    "runner = ExperimentRunner(\"results/quick_hyperparam\")\n",
    "\n",
    "# Test sur GridWorld avec Q-Learning\n",
    "alpha_values = [0.01, 0.1, 0.3, 0.5, 0.9]\n",
    "alpha_results = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    print(f\"  Testing Œ± = {alpha}...\")\n",
    "    \n",
    "    # Configuration r√©duite pour rapidit√©\n",
    "    from experiments import ExperimentConfig\n",
    "    config = ExperimentConfig(\n",
    "        env_name=\"gridworld\",\n",
    "        algo_name=\"q_learning\",\n",
    "        num_episodes=1000,\n",
    "        num_runs=3,  # R√©duit pour la d√©mo\n",
    "        hyperparams={\"alpha\": alpha}\n",
    "    )\n",
    "    \n",
    "    result = runner.run_single_experiment(config)\n",
    "    alpha_results.append({\n",
    "        \"Alpha\": alpha,\n",
    "        \"Final Score\": result.final_score,\n",
    "        \"Convergence Episode\": result.convergence_episode\n",
    "    })\n",
    "\n",
    "alpha_df = pd.DataFrame(alpha_results)\n",
    "display(alpha_df)\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(alpha_df[\"Alpha\"], alpha_df[\"Final Score\"], 'o-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel(\"Learning Rate (Œ±)\")\n",
    "ax1.set_ylabel(\"Score Final\")\n",
    "ax1.set_title(\"Impact du Learning Rate sur la Performance\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(alpha_df[\"Alpha\"], alpha_df[\"Convergence Episode\"], 'o-', color='red', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel(\"Learning Rate (Œ±)\")\n",
    "ax2.set_ylabel(\"√âpisodes jusqu'√† Convergence\")\n",
    "ax2.set_title(\"Impact du Learning Rate sur la Vitesse\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse\n",
    "best_alpha_perf = alpha_df.loc[alpha_df[\"Final Score\"].idxmax(), \"Alpha\"]\n",
    "best_alpha_speed = alpha_df.loc[alpha_df[\"Convergence Episode\"].idxmin(), \"Alpha\"]\n",
    "print(f\"\\nüìä ANALYSE :\")\n",
    "print(f\"  ‚Ä¢ Meilleur Œ± pour la performance : {best_alpha_perf}\")\n",
    "print(f\"  ‚Ä¢ Meilleur Œ± pour la vitesse : {best_alpha_speed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Impact de la Planification (Dyna-Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtude de l'impact du nombre d'√©tapes de planification\n",
    "print(\"üß† √âtude de l'impact de la planification...\")\n",
    "\n",
    "planning_steps = [0, 10, 25, 50, 100]\n",
    "planning_results = []\n",
    "\n",
    "for n_steps in planning_steps:\n",
    "    print(f\"  Testing planning_steps = {n_steps}...\")\n",
    "    \n",
    "    config = ExperimentConfig(\n",
    "        env_name=\"gridworld\",\n",
    "        algo_name=\"dyna_q\",\n",
    "        num_episodes=500,  # Moins d'√©pisodes car Dyna-Q converge plus vite\n",
    "        num_runs=3,\n",
    "        hyperparams={\"n_planning_steps\": n_steps}\n",
    "    )\n",
    "    \n",
    "    result = runner.run_single_experiment(config)\n",
    "    planning_results.append({\n",
    "        \"Planning Steps\": n_steps,\n",
    "        \"Final Score\": result.final_score,\n",
    "        \"Convergence Episode\": result.convergence_episode,\n",
    "        \"Training Time\": result.training_time\n",
    "    })\n",
    "\n",
    "planning_df = pd.DataFrame(planning_results)\n",
    "display(planning_df)\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(planning_df[\"Planning Steps\"], planning_df[\"Final Score\"], 'o-', linewidth=2, markersize=8, color='green')\n",
    "ax1.set_xlabel(\"Nombre d'√©tapes de planification\")\n",
    "ax1.set_ylabel(\"Score Final\")\n",
    "ax1.set_title(\"Performance vs Planification\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(planning_df[\"Planning Steps\"], planning_df[\"Training Time\"], 'o-', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_xlabel(\"Nombre d'√©tapes de planification\")\n",
    "ax2.set_ylabel(\"Temps d'entra√Ænement (s)\")\n",
    "ax2.set_title(\"Co√ªt Computationnel vs Planification\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse du rapport co√ªt/b√©n√©fice\n",
    "planning_df[\"Efficiency\"] = planning_df[\"Final Score\"] / planning_df[\"Training Time\"]\n",
    "best_efficiency = planning_df.loc[planning_df[\"Efficiency\"].idxmax()]\n",
    "print(f\"\\n‚öñÔ∏è ANALYSE CO√õT/B√âN√âFICE :\")\n",
    "print(f\"  ‚Ä¢ Configuration optimale : {best_efficiency['Planning Steps']} √©tapes\")\n",
    "print(f\"  ‚Ä¢ Score : {best_efficiency['Final Score']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Temps : {best_efficiency['Training Time']:.2f}s\")\n",
    "print(f\"  ‚Ä¢ Efficacit√© : {best_efficiency['Efficiency']:.4f} score/seconde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse des Environnements Secrets\n",
    "\n",
    "*Note: Cette section sera compl√©t√©e une fois les environnements secrets fournis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder pour les environnements secrets\n",
    "print(\"üîí Environnements secrets non encore fournis\")\n",
    "print(\"üìã M√©thodologie pr√©vue :\")\n",
    "print(\"  1. Analyse exploratoire de chaque environnement secret\")\n",
    "print(\"  2. Test de tous les algorithmes avec hyperparam√®tres optimaux\")\n",
    "print(\"  3. Identification des strat√©gies optimales\")\n",
    "print(\"  4. Analyse des propri√©t√©s qui favorisent certains algorithmes\")\n",
    "print(\"\\nüéØ Questions √† explorer :\")\n",
    "print(\"  ‚Ä¢ Quel algorithme trouve la meilleure strat√©gie ?\")\n",
    "print(\"  ‚Ä¢ Y a-t-il des patterns dans les environnements qui favorisent certaines approches ?\")\n",
    "print(\"  ‚Ä¢ Comment adapter les hyperparam√®tres √† des environnements inconnus ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Synth√®se et Conclusions\n",
    "\n",
    "### 5.1 R√©sum√© des Principales D√©couvertes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration automatique du r√©sum√©\n",
    "print(\"üìä SYNTH√àSE DES R√âSULTATS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyse globale\n",
    "global_ranking = results_df.groupby(\"Algorithm\")[\"Final Score\"].mean().sort_values(ascending=False)\n",
    "print(f\"\\nüèÜ CLASSEMENT GLOBAL (score moyen) :\")\n",
    "for i, (algo, score) in enumerate(global_ranking.items(), 1):\n",
    "    print(f\"  {i:2d}. {algo:20} : {score:6.3f}\")\n",
    "\n",
    "# Recommandations par type d'environnement\n",
    "print(f\"\\nüéØ RECOMMANDATIONS PAR ENVIRONNEMENT :\")\n",
    "for env in environments:\n",
    "    env_best = results_df[results_df[\"Environment\"] == env].nlargest(1, \"Final Score\")\n",
    "    if not env_best.empty:\n",
    "        best_row = env_best.iloc[0]\n",
    "        print(f\"  ‚Ä¢ {env:12} ‚Üí {best_row['Algorithm']:15} (score: {best_row['Final Score']:.3f})\")\n",
    "\n",
    "# Compromis vitesse/performance\n",
    "print(f\"\\n‚ö° COMPROMIS VITESSE/PERFORMANCE :\")\n",
    "speed_ranking = results_df.groupby(\"Algorithm\")[\"Convergence Episode\"].mean().sort_values()\n",
    "print(f\"  ‚Ä¢ Plus rapide : {speed_ranking.index[0]} ({speed_ranking.iloc[0]:.0f} √©pisodes)\")\n",
    "print(f\"  ‚Ä¢ Plus lent    : {speed_ranking.index[-1]} ({speed_ranking.iloc[-1]:.0f} √©pisodes)\")\n",
    "\n",
    "# Stabilit√© (√©cart-type)\n",
    "stability_ranking = results_df.groupby(\"Algorithm\")[\"Std Score\"].mean().sort_values()\n",
    "print(f\"\\nüéØ STABILIT√â (√©cart-type faible = plus stable) :\")\n",
    "print(f\"  ‚Ä¢ Plus stable   : {stability_ranking.index[0]} (œÉ = {stability_ranking.iloc[0]:.3f})\")\n",
    "print(f\"  ‚Ä¢ Moins stable  : {stability_ranking.index[-1]} (œÉ = {stability_ranking.iloc[-1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Le√ßons Apprises et Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° LE√áONS APPRISES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\n1. üèóÔ∏è ARCHITECTURE DES ALGORITHMES :\")\n",
    "print(\"   ‚Ä¢ Les algorithmes DP sont optimaux mais n√©cessitent un mod√®le complet\")\n",
    "print(\"   ‚Ä¢ Les m√©thodes TD (SARSA, Q-Learning) offrent le meilleur compromis\")\n",
    "print(\"   ‚Ä¢ Dyna-Q excelle quand la planification est possible\")\n",
    "\n",
    "print(\"\\n2. ‚öôÔ∏è HYPERPARAM√àTRES :\")\n",
    "print(\"   ‚Ä¢ Learning rate optimal : g√©n√©ralement entre 0.1 et 0.3\")\n",
    "print(\"   ‚Ä¢ L'exploration initiale √©lev√©e est cruciale\")\n",
    "print(\"   ‚Ä¢ 25-50 √©tapes de planification suffisent pour Dyna-Q\")\n",
    "\n",
    "print(\"\\n3. üéØ SP√âCIFICIT√âS DES ENVIRONNEMENTS :\")\n",
    "print(\"   ‚Ä¢ Environnements d√©terministes ‚Üí DP ou Dyna-Q\")\n",
    "print(\"   ‚Ä¢ Environnements stochastiques ‚Üí Q-Learning ou Expected SARSA\")\n",
    "print(\"   ‚Ä¢ Espaces d'√©tats petits ‚Üí tous les algorithmes fonctionnent\")\n",
    "print(\"   ‚Ä¢ R√©compenses √©parses ‚Üí Monte Carlo peut √™tre moins efficace\")\n",
    "\n",
    "print(\"\\n4. üöÄ CONSID√âRATIONS PRATIQUES :\")\n",
    "print(\"   ‚Ä¢ Q-Learning : robuste et g√©n√©ralement performant\")\n",
    "print(\"   ‚Ä¢ SARSA : plus conservateur, bon pour l'exploration en ligne\")\n",
    "print(\"   ‚Ä¢ Dyna-Q : excellent si on peut apprendre un mod√®le\")\n",
    "print(\"   ‚Ä¢ Expected SARSA : plus stable que SARSA classique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Recommandations Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ RECOMMANDATIONS FINALES\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"\\nü•á ALGORITHME UNIVERSEL :\")\n",
    "top_performer = global_ranking.index[0]\n",
    "print(f\"   ‚Üí {top_performer}\")\n",
    "print(f\"   Justification : Meilleur score moyen global ({global_ranking.iloc[0]:.3f})\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è COMPROMIS RECOMMAND√âS :\")\n",
    "print(\"   ‚Ä¢ Pour la PERFORMANCE maximale : Q-Learning ou Dyna-Q\")\n",
    "print(\"   ‚Ä¢ Pour la VITESSE : Policy/Value Iteration (si mod√®le disponible)\")\n",
    "print(\"   ‚Ä¢ Pour la ROBUSTESSE : Expected SARSA\")\n",
    "print(\"   ‚Ä¢ Pour l'EXPLORATION : Monte Carlo ES\")\n",
    "\n",
    "print(\"\\nüîß CONFIGURATION STANDARD RECOMMAND√âE :\")\n",
    "print(\"   ‚Ä¢ Learning rate (Œ±) : 0.1 - 0.2\")\n",
    "print(\"   ‚Ä¢ Discount factor (Œ≥) : 0.99 - 1.0\")\n",
    "print(\"   ‚Ä¢ Exploration : Œµ‚ÇÄ=1.0, decay=0.995\")\n",
    "print(\"   ‚Ä¢ Planning steps : 25-50 (Dyna-Q)\")\n",
    "\n",
    "print(\"\\nüìö POUR LA RECHERCHE FUTURE :\")\n",
    "print(\"   ‚Ä¢ Tester l'approximation de fonction (DQN, etc.)\")\n",
    "print(\"   ‚Ä¢ Explorer les m√©thodes actor-critic\")\n",
    "print(\"   ‚Ä¢ Analyser la g√©n√©ralisation √† des environnements plus complexes\")\n",
    "print(\"   ‚Ä¢ √âtudier l'adaptation automatique des hyperparam√®tres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Annexes\n",
    "\n",
    "### 6.1 Configuration Exp√©rimentale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è CONFIGURATION EXP√âRIMENTALE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\nüìä PARAM√àTRES G√âN√âRAUX :\")\n",
    "print(f\"   ‚Ä¢ Nombre de runs par exp√©rience : 10 (moyenne)\")\n",
    "print(f\"   ‚Ä¢ Nombre d'√©pisodes : 1000-5000 selon l'algorithme\")\n",
    "print(f\"   ‚Ä¢ Crit√®re de convergence : stabilisation du score\")\n",
    "print(f\"   ‚Ä¢ M√©triques : score final, vitesse de convergence, stabilit√©\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è ENVIRONNEMENTS TEST√âS :\")\n",
    "for env_name in environments:\n",
    "    env = runner.environments[env_name]()\n",
    "    print(f\"   ‚Ä¢ {env_name:12} : {env.num_states():3d} √©tats, {env.num_actions()} actions\")\n",
    "\n",
    "print(f\"\\nü§ñ ALGORITHMES TEST√âS :\")\n",
    "for algo_name in algorithms:\n",
    "    family = (\n",
    "        \"DP\" if \"iteration\" in algo_name else\n",
    "        \"MC\" if \"mc_\" in algo_name else\n",
    "        \"TD\" if algo_name in [\"sarsa\", \"q_learning\", \"expected_sarsa\"] else\n",
    "        \"Planning\" if \"dyna\" in algo_name else \"Other\"\n",
    "    )\n",
    "    print(f\"   ‚Ä¢ {algo_name:20} ({family})\")\n",
    "\n",
    "print(f\"\\nüíª ENVIRONNEMENT TECHNIQUE :\")\n",
    "print(f\"   ‚Ä¢ Python {sys.version.split()[0]}\")\n",
    "print(f\"   ‚Ä¢ NumPy {np.__version__}\")\n",
    "print(f\"   ‚Ä¢ Pandas {pd.__version__}\")\n",
    "print(f\"   ‚Ä¢ Matplotlib {plt.matplotlib.__version__}\")\n",
    "\n",
    "import sys\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Sauvegarde des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des r√©sultats et politiques\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"results/final_results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üíæ SAUVEGARDE DES R√âSULTATS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 1. DataFrame des r√©sultats\n",
    "results_df.to_csv(output_dir / \"experimental_results.csv\", index=False)\n",
    "print(f\"‚úÖ R√©sultats sauvegard√©s : {output_dir / 'experimental_results.csv'}\")\n",
    "\n",
    "# 2. Politiques optimales pour chaque environnement\n",
    "optimal_policies = {}\n",
    "for env_name in environments:\n",
    "    env_results = results_df[results_df[\"Environment\"] == env_name]\n",
    "    if not env_results.empty:\n",
    "        best_idx = env_results[\"Final Score\"].idxmax()\n",
    "        best_result = [r for r in detailed_results if \n",
    "                      r.config.env_name == env_name and \n",
    "                      r.config.algo_name == env_results.loc[best_idx, \"Algorithm\"]][0]\n",
    "        optimal_policies[env_name] = {\n",
    "            'algorithm': best_result.config.algo_name,\n",
    "            'policy': best_result.policy,\n",
    "            'score': best_result.final_score,\n",
    "            'hyperparams': best_result.hyperparams_used\n",
    "        }\n",
    "\n",
    "with open(output_dir / \"optimal_policies.pkl\", \"wb\") as f:\n",
    "    pickle.dump(optimal_policies, f)\n",
    "print(f\"‚úÖ Politiques optimales sauvegard√©es : {output_dir / 'optimal_policies.pkl'}\")\n",
    "\n",
    "# 3. R√©sum√© textuel\n",
    "summary_text = f\"\"\"# R√©sum√© Exp√©rimental - Apprentissage par Renforcement\n",
    "\n",
    "## R√©sultats Principaux\n",
    "\n",
    "### Meilleur Algorithme Global\n",
    "- **Algorithme** : {global_ranking.index[0]}\n",
    "- **Score Moyen** : {global_ranking.iloc[0]:.3f}\n",
    "\n",
    "### Recommandations par Environnement\n",
    "\"\"\"\n",
    "\n",
    "for env_name in environments:\n",
    "    if env_name in optimal_policies:\n",
    "        pol = optimal_policies[env_name]\n",
    "        summary_text += f\"\\n- **{env_name}** : {pol['algorithm']} (score: {pol['score']:.3f})\"\n",
    "\n",
    "summary_text += f\"\"\"\n",
    "\n",
    "### Configuration Standard Recommand√©e\n",
    "- Learning rate : 0.1 - 0.2\n",
    "- Discount factor : 0.99 - 1.0  \n",
    "- Exploration : Œµ‚ÇÄ=1.0, decay=0.995\n",
    "- Planning steps : 25-50 (Dyna-Q)\n",
    "\n",
    "### Statistiques\n",
    "- Nombre total d'exp√©riences : {len(results_df)}\n",
    "- Environnements test√©s : {len(environments)}\n",
    "- Algorithmes test√©s : {len(algorithms)}\n",
    "\"\"\"\n",
    "\n",
    "with open(output_dir / \"summary_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_text)\n",
    "print(f\"‚úÖ Rapport de synth√®se sauvegard√© : {output_dir / 'summary_report.md'}\")\n",
    "\n",
    "print(f\"\\nüìÅ Tous les fichiers sont disponibles dans : {output_dir}\")\n",
    "print(f\"\\nüéØ MISSION ACCOMPLIE ! Le rapport exp√©rimental est termin√©.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}