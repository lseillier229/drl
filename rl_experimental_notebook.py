{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapport Exp√©rimental - Algorithmes d'Apprentissage par Renforcement\n",
    "\n",
    "**Projet**: (Deep) Reinforcement Learning P1  \n",
    "**Ann√©e**: 2024-2025  \n",
    "**Enseignant**: Nicolas VIDAL (nvidal@myges.fr)\n",
    "\n",
    "---\n",
    "\n",
    "## Table des Mati√®res\n",
    "\n",
    "1. [Introduction et Objectifs](#introduction)\n",
    "2. [Environnements Impl√©ment√©s](#environnements)\n",
    "3. [Algorithmes Impl√©ment√©s](#algorithmes)\n",
    "4. [Exp√©rimentations et R√©sultats](#experimentations)\n",
    "5. [√âtude des Hyperparam√®tres](#hyperparametres)\n",
    "6. [Analyse Comparative](#analyse)\n",
    "7. [Environnements Secrets](#secrets)\n",
    "8. [Conclusions et Recommandations](#conclusions)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction et Objectifs <a id=\"introduction\"></a>\n",
    "\n",
    "Ce projet vise √† impl√©menter et comparer les algorithmes classiques d'apprentissage par renforcement :\n",
    "\n",
    "- **Dynamic Programming** : Policy Iteration, Value Iteration\n",
    "- **Monte Carlo** : ES, On-policy, Off-policy\n",
    "- **Temporal Difference** : SARSA, Q-Learning, Expected SARSA\n",
    "- **Planning** : Dyna-Q, Dyna-Q+\n",
    "\n",
    "### Questions de recherche\n",
    "\n",
    "1. Quel algorithme est le plus performant sur quel environnement ?\n",
    "2. Comment les hyperparam√®tres affectent-ils les performances ?\n",
    "3. Quels sont les compromis vitesse/qualit√© pour chaque m√©thode ?\n",
    "4. Comment adapter les strat√©gies aux environnements inconnus ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Imports du projet\n",
    "from envs import LineWorld, GridWorld, RPS, MontyHall1, MontyHall2\n",
    "from algos import (\n",
    "    policy_iteration, value_iteration, build_model_from_env,\n",
    "    mc_control_es, on_policy_first_visit_mc_control, off_policy_mc_control,\n",
    "    sarsa, q_learning, expected_sarsa,\n",
    "    dyna_q, dyna_q_plus\n",
    ")\n",
    "from visualization import EnvironmentVisualizer, PolicyManager\n",
    "\n",
    "print(\"‚úÖ Setup termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environnements Impl√©ment√©s <a id=\"environnements\"></a>\n",
    "\n",
    "### 2.1 LineWorld\n",
    "\n",
    "Un monde lin√©aire simple avec 5 √©tats. L'agent commence au milieu et doit atteindre l'extr√©mit√© droite (+1) en √©vitant l'extr√©mit√© gauche (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration LineWorld\n",
    "env = LineWorld()\n",
    "print(f\"üìè LineWorld : {env.num_states()} √©tats, {env.num_actions()} actions\")\n",
    "print(f\"Actions : 0=Gauche, 1=Droite\")\n",
    "print(f\"\\nSimulation d'un √©pisode optimal :\")\n",
    "\n",
    "env.reset()\n",
    "visualizer = EnvironmentVisualizer(env)\n",
    "visualizer.visualize_state()\n",
    "plt.title(\"LineWorld - √âtat Initial\")\n",
    "plt.show()\n",
    "\n",
    "# Strat√©gie optimale : toujours aller √† droite\n",
    "steps = 0\n",
    "while not env.is_game_over() and steps < 10:\n",
    "    action = 1  # Droite\n",
    "    env.step(action)\n",
    "    steps += 1\n",
    "\n",
    "print(f\"\\nüèÅ Score final : {env.score()}, en {steps} √©tapes\")\n",
    "print(f\"üí° Strat√©gie optimale : toujours aller √† droite (action=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GridWorld\n",
    "\n",
    "Une grille 5√ó5 avec deux √©tats terminaux : un pi√®ge en (0,4) avec r√©compense -3, et un objectif en (4,4) avec r√©compense +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration GridWorld\n",
    "env = GridWorld()\n",
    "print(f\"üèóÔ∏è GridWorld : {env.num_states()} √©tats, {env.num_actions()} actions\")\n",
    "print(f\"Actions : 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\")\n",
    "\n",
    "# Visualisation de l'environnement\n",
    "env.reset()\n",
    "visualizer = EnvironmentVisualizer(env)\n",
    "visualizer.visualize_state()\n",
    "plt.title(\"GridWorld - Configuration\")\n",
    "plt.show()\n",
    "\n",
    "# Chemin optimal\n",
    "print(\"\\nüí° Strat√©gie optimale : Aller vers le coin inf√©rieur droit en √©vitant le pi√®ge\")\n",
    "print(\"   Chemin sugg√©r√© : 4√óRIGHT + 4√óDOWN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Rock-Paper-Scissors (RPS)\n",
    "\n",
    "Jeu en 2 manches :\n",
    "- Manche 1 : l'adversaire joue al√©atoirement\n",
    "- Manche 2 : l'adversaire copie votre coup de la manche 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse du RPS\n",
    "env = RPS()\n",
    "print(f\"‚úÇÔ∏è Rock-Paper-Scissors : {env.num_states()} √©tats, {env.num_actions()} actions\")\n",
    "print(f\"Actions : 0=ROCK, 1=PAPER, 2=SCISSORS\")\n",
    "\n",
    "# Simulation de la strat√©gie optimale\n",
    "print(\"\\nüéØ Analyse de la strat√©gie optimale :\")\n",
    "print(\"1. Manche 1 : Peu importe (adversaire al√©atoire)\")\n",
    "print(\"2. Manche 2 : Jouer ce qui bat notre coup de manche 1\")\n",
    "print(\"\\nExemple : Si on joue ROCK en manche 1, jouer PAPER en manche 2\")\n",
    "\n",
    "# Test de la strat√©gie\n",
    "wins = 0\n",
    "num_tests = 1000\n",
    "for _ in range(num_tests):\n",
    "    env.reset()\n",
    "    # Manche 1 : jouer ROCK\n",
    "    env.step(0)\n",
    "    # Manche 2 : jouer PAPER (qui bat ROCK)\n",
    "    env.step(1)\n",
    "    if env.score() > 0:\n",
    "        wins += 1\n",
    "\n",
    "print(f\"\\nüìä R√©sultats sur {num_tests} parties : {wins/num_tests:.1%} de victoires\")\n",
    "print(f\"   (Attendu : ~50% car la manche 1 est al√©atoire)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Monty Hall\n",
    "\n",
    "Le c√©l√®bre paradoxe de Monty Hall, impl√©ment√© avec 3 portes (MontyHall1) et 5 portes (MontyHall2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse du paradoxe de Monty Hall\n",
    "env = MontyHall1()\n",
    "print(f\"üö™ Monty Hall (3 portes) : {env.num_states()} √©tats, {env.num_actions()} actions\")\n",
    "\n",
    "# Test des deux strat√©gies\n",
    "strategies = {\n",
    "    \"Toujours garder\": lambda first_choice, available: first_choice,\n",
    "    \"Toujours changer\": lambda first_choice, available: available[0] if available[0] != first_choice else available[1]\n",
    "}\n",
    "\n",
    "results = {}\n",
    "num_simulations = 10000\n",
    "\n",
    "for strategy_name, strategy_func in strategies.items():\n",
    "    wins = 0\n",
    "    for _ in range(num_simulations):\n",
    "        env.reset()\n",
    "        \n",
    "        # Premi√®re action : choisir une porte (ex: porte 0)\n",
    "        first_choice = 0\n",
    "        env.step(first_choice)\n",
    "        \n",
    "        # Deuxi√®me action : appliquer la strat√©gie\n",
    "        # Dans notre impl√©mentation simplifi√©e, on suppose que Monty ouvre toujours une porte\n",
    "        # et qu'il reste 2 choix possibles\n",
    "        if strategy_name == \"Toujours changer\":\n",
    "            # Changer pour une autre porte\n",
    "            env.step(1 if first_choice != 1 else 2)\n",
    "        else:\n",
    "            # Garder la m√™me porte\n",
    "            env.step(first_choice)\n",
    "        \n",
    "        if env.score() > 0:\n",
    "            wins += 1\n",
    "    \n",
    "    results[strategy_name] = wins / num_simulations\n",
    "\n",
    "print(\"\\nüìä R√©sultats des strat√©gies :\")\n",
    "for strategy, win_rate in results.items():\n",
    "    print(f\"   {strategy:20} : {win_rate:.1%}\")\n",
    "print(\"\\nüí° Conclusion : Il vaut mieux toujours changer de porte !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithmes Impl√©ment√©s <a id=\"algorithmes\"></a>\n",
    "\n",
    "### Vue d'ensemble des familles d'algorithmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau r√©capitulatif des algorithmes\n",
    "algorithms_info = pd.DataFrame([\n",
    "    {\"Famille\": \"Dynamic Programming\", \"Algorithme\": \"Policy Iteration\", \n",
    "     \"Type\": \"Model-based\", \"Complexit√©\": \"O(S¬≤A)\", \"Convergence\": \"Garantie\"},\n",
    "    {\"Famille\": \"Dynamic Programming\", \"Algorithme\": \"Value Iteration\", \n",
    "     \"Type\": \"Model-based\", \"Complexit√©\": \"O(S¬≤A)\", \"Convergence\": \"Garantie\"},\n",
    "    {\"Famille\": \"Monte Carlo\", \"Algorithme\": \"MC ES\", \n",
    "     \"Type\": \"Model-free\", \"Complexit√©\": \"O(episodes)\", \"Convergence\": \"Asymptotique\"},\n",
    "    {\"Famille\": \"Monte Carlo\", \"Algorithme\": \"On-policy MC\", \n",
    "     \"Type\": \"Model-free\", \"Complexit√©\": \"O(episodes)\", \"Convergence\": \"Asymptotique\"},\n",
    "    {\"Famille\": \"Monte Carlo\", \"Algorithme\": \"Off-policy MC\", \n",
    "     \"Type\": \"Model-free\", \"Complexit√©\": \"O(episodes)\", \"Convergence\": \"Asymptotique\"},\n",
    "    {\"Famille\": \"Temporal Difference\", \"Algorithme\": \"SARSA\", \n",
    "     \"Type\": \"Model-free\", \"Complexit√©\": \"O(1) par step\", \"Convergence\": \"Asymptotique\"},\n",
    "    {\"Famille\": \"Temporal Difference\", \"Algorithme\": \"Q-Learning\", \n",
    "     \"Type\": \"Model-free\", \"Complexit√©\": \"O(1) par step\", \"Convergence\": \"Garantie*\"},\n",
    "    {\"Famille\": \"Temporal Difference\", \"Algorithme\": \"Expected SARSA\", \n",
    "     \"Type\": \"Model-free\", \"Complexit√©\": \"O(A) par step\", \"Convergence\": \"Asymptotique\"},\n",
    "    {\"Famille\": \"Planning\", \"Algorithme\": \"Dyna-Q\", \n",
    "     \"Type\": \"Model-based\", \"Complexit√©\": \"O(n) par step\", \"Convergence\": \"Asymptotique\"},\n",
    "    {\"Famille\": \"Planning\", \"Algorithme\": \"Dyna-Q+\", \n",
    "     \"Type\": \"Model-based\", \"Complexit√©\": \"O(n) par step\", \"Convergence\": \"Asymptotique\"},\n",
    "])\n",
    "\n",
    "print(\"üìä Tableau comparatif des algorithmes :\")\n",
    "display(algorithms_info)\n",
    "\n",
    "print(\"\\n* Q-Learning converge vers Q* sous conditions (exploration infinie, learning rate d√©croissant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exp√©rimentations et R√©sultats <a id=\"experimentations\"></a>\n",
    "\n",
    "### 4.1 Protocole exp√©rimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition du protocole exp√©rimental\n",
    "class ExperimentRunner:\n",
    "    def __init__(self):\n",
    "        self.environments = {\n",
    "            'lineworld': LineWorld,\n",
    "            'gridworld': GridWorld,\n",
    "            'rps': RPS,\n",
    "            'montyhall1': MontyHall1\n",
    "        }\n",
    "        \n",
    "        self.algorithms = {\n",
    "            'policy_iteration': self.run_policy_iteration,\n",
    "            'value_iteration': self.run_value_iteration,\n",
    "            'mc_es': self.run_mc_es,\n",
    "            'mc_on_policy': self.run_mc_on_policy,\n",
    "            'mc_off_policy': self.run_mc_off_policy,\n",
    "            'sarsa': self.run_sarsa,\n",
    "            'q_learning': self.run_q_learning,\n",
    "            'expected_sarsa': self.run_expected_sarsa,\n",
    "            'dyna_q': self.run_dyna_q\n",
    "        }\n",
    "        \n",
    "        self.default_params = {\n",
    "            'num_episodes': 1000,\n",
    "            'gamma': 1.0,\n",
    "            'alpha': 0.1,\n",
    "            'epsilon_start': 1.0,\n",
    "            'epsilon_decay': 0.995,\n",
    "            'n_planning_steps': 50\n",
    "        }\n",
    "    \n",
    "    def evaluate_policy(self, env, policy, num_episodes=100):\n",
    "        \"\"\"√âvalue une politique sur plusieurs √©pisodes.\"\"\"\n",
    "        scores = []\n",
    "        for _ in range(num_episodes):\n",
    "            env.reset()\n",
    "            steps = 0\n",
    "            while not env.is_game_over() and steps < 1000:\n",
    "                state = env.state()\n",
    "                if policy.ndim == 2:\n",
    "                    action = np.random.choice(env.num_actions(), p=policy[state])\n",
    "                else:\n",
    "                    action = int(policy[state])\n",
    "                env.step(action)\n",
    "                steps += 1\n",
    "            scores.append(env.score())\n",
    "        return np.mean(scores), np.std(scores)\n",
    "    \n",
    "    def run_policy_iteration(self, env, params):\n",
    "        S, A, R_vals, T, p = build_model_from_env(env)\n",
    "        pi, V = policy_iteration(S, A, R_vals, T, p, gamma=params['gamma'])\n",
    "        # Convertir en politique stochastique pour l'√©valuation\n",
    "        policy = np.zeros((len(S), len(A)))\n",
    "        for s in range(len(S)):\n",
    "            policy[s, pi[s]] = 1.0\n",
    "        return policy, V, None\n",
    "    \n",
    "    def run_value_iteration(self, env, params):\n",
    "        S, A, R_vals, T, p = build_model_from_env(env)\n",
    "        V = value_iteration(S, A, R_vals, T, p, gamma=params['gamma'])\n",
    "        # Extraire la politique greedy\n",
    "        policy = np.zeros((len(S), len(A)))\n",
    "        for s in S:\n",
    "            if s not in T:\n",
    "                q_values = []\n",
    "                for a in A:\n",
    "                    q = sum(p[s, a, s_p, r_idx] * (R_vals[r_idx] + params['gamma'] * V[s_p])\n",
    "                           for s_p in S for r_idx in range(len(R_vals)))\n",
    "                    q_values.append(q)\n",
    "                best_action = np.argmax(q_values)\n",
    "                policy[s, best_action] = 1.0\n",
    "            else:\n",
    "                policy[s, :] = 1.0 / len(A)  # Action uniforme dans les √©tats terminaux\n",
    "        return policy, V, None\n",
    "    \n",
    "    def run_mc_es(self, env, params):\n",
    "        pi, Q = mc_control_es(env, num_episodes=params['num_episodes'], gamma=params['gamma'])\n",
    "        return pi, None, Q\n",
    "    \n",
    "    def run_mc_on_policy(self, env, params):\n",
    "        pi, Q = on_policy_first_visit_mc_control(\n",
    "            env, num_episodes=params['num_episodes'], gamma=params['gamma'],\n",
    "            epsilon_start=params['epsilon_start'], epsilon_decay=params['epsilon_decay']\n",
    "        )\n",
    "        return pi, None, Q\n",
    "    \n",
    "    def run_mc_off_policy(self, env, params):\n",
    "        pi, Q = off_policy_mc_control(env, num_episodes=params['num_episodes'], gamma=params['gamma'])\n",
    "        return pi, None, Q\n",
    "    \n",
    "    def run_sarsa(self, env, params):\n",
    "        pi, Q = sarsa(\n",
    "            env, num_episodes=params['num_episodes'], alpha=params['alpha'],\n",
    "            gamma=params['gamma'], epsilon_start=params['epsilon_start'],\n",
    "            epsilon_decay=params['epsilon_decay']\n",
    "        )\n",
    "        return pi, None, Q\n",
    "    \n",
    "    def run_q_learning(self, env, params):\n",
    "        pi, Q = q_learning(\n",
    "            env, num_episodes=params['num_episodes'], alpha=params['alpha'],\n",
    "            gamma=params['gamma'], epsilon_start=params['epsilon_start'],\n",
    "            epsilon_decay=params['epsilon_decay']\n",
    "        )\n",
    "        return pi, None, Q\n",
    "    \n",
    "    def run_expected_sarsa(self, env, params):\n",
    "        pi, Q = expected_sarsa(\n",
    "            env, num_episodes=params['num_episodes'], alpha=params['alpha'],\n",
    "            gamma=params['gamma'], epsilon_start=params['epsilon_start'],\n",
    "            epsilon_decay=params['epsilon_decay']\n",
    "        )\n",
    "        return pi, None, Q\n",
    "    \n",
    "    def run_dyna_q(self, env, params):\n",
    "        pi, Q, model = dyna_q(\n",
    "            env, num_episodes=params['num_episodes'], \n",
    "            n_planning_steps=params['n_planning_steps'],\n",
    "            alpha=params['alpha'], gamma=params['gamma'],\n",
    "            epsilon_start=params['epsilon_start'], epsilon_decay=params['epsilon_decay']\n",
    "        )\n",
    "        return pi, None, Q\n",
    "    \n",
    "    def run_experiment(self, env_name, algo_name, params=None):\n",
    "        \"\"\"Ex√©cute une exp√©rience compl√®te.\"\"\"\n",
    "        if params is None:\n",
    "            params = self.default_params.copy()\n",
    "        \n",
    "        env = self.environments[env_name]()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        policy, V, Q = self.algorithms[algo_name](env, params)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        mean_score, std_score = self.evaluate_policy(env, policy)\n",
    "        \n",
    "        return {\n",
    "            'environment': env_name,\n",
    "            'algorithm': algo_name,\n",
    "            'mean_score': mean_score,\n",
    "            'std_score': std_score,\n",
    "            'training_time': training_time,\n",
    "            'policy': policy,\n",
    "            'V': V,\n",
    "            'Q': Q,\n",
    "            'params': params\n",
    "        }\n",
    "\n",
    "# Initialiser le runner\n",
    "runner = ExperimentRunner()\n",
    "print(\"‚úÖ ExperimentRunner initialis√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Ex√©cution des exp√©riences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex√©cution de toutes les exp√©riences\n",
    "print(\"üöÄ Lancement des exp√©riences...\")\n",
    "print(\"‚è±Ô∏è Cela peut prendre quelques minutes...\\n\")\n",
    "\n",
    "results = []\n",
    "environments = ['lineworld', 'gridworld', 'rps', 'montyhall1']\n",
    "algorithms = ['policy_iteration', 'value_iteration', 'mc_es', 'mc_on_policy', \n",
    "              'mc_off_policy', 'sarsa', 'q_learning', 'expected_sarsa', 'dyna_q']\n",
    "\n",
    "# Pour chaque combinaison environnement/algorithme\n",
    "for env_name in environments:\n",
    "    print(f\"\\nüìç Environnement : {env_name}\")\n",
    "    \n",
    "    for algo_name in algorithms:\n",
    "        try:\n",
    "            print(f\"   ‚Ä¢ {algo_name:20} : \", end='', flush=True)\n",
    "            \n",
    "            # Certains algorithmes n√©cessitent moins d'√©pisodes\n",
    "            params = runner.default_params.copy()\n",
    "            if algo_name in ['policy_iteration', 'value_iteration']:\n",
    "                params['num_episodes'] = 1  # Pas besoin d'√©pisodes pour DP\n",
    "            elif algo_name == 'dyna_q':\n",
    "                params['num_episodes'] = 500  # Dyna-Q converge plus vite\n",
    "            \n",
    "            # Skip certaines combinaisons probl√©matiques\n",
    "            if algo_name in ['policy_iteration', 'value_iteration'] and env_name in ['rps', 'montyhall1']:\n",
    "                print(\"SKIPPED (trop complexe pour DP)\")\n",
    "                continue\n",
    "            \n",
    "            result = runner.run_experiment(env_name, algo_name, params)\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"Score: {result['mean_score']:6.3f} ¬± {result['std_score']:5.3f}, \"\n",
    "                  f\"Time: {result['training_time']:5.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERREUR: {str(e)[:50]}...\")\n",
    "\n",
    "# Cr√©er un DataFrame avec les r√©sultats\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Environment': r['environment'],\n",
    "        'Algorithm': r['algorithm'],\n",
    "        'Mean Score': r['mean_score'],\n",
    "        'Std Score': r['std_score'],\n",
    "        'Training Time': r['training_time']\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "print(f\"\\n‚úÖ {len(results)} exp√©riences compl√©t√©es !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analyse des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau r√©capitulatif des performances\n",
    "print(\"üìä TABLEAU R√âCAPITULATIF DES PERFORMANCES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pivot table pour une meilleure lisibilit√©\n",
    "pivot_scores = results_df.pivot_table(\n",
    "    values='Mean Score', \n",
    "    index='Algorithm', \n",
    "    columns='Environment',\n",
    "    aggfunc='mean'\n",
    ").round(3)\n",
    "\n",
    "display(pivot_scores)\n",
    "\n",
    "# Meilleur algorithme par environnement\n",
    "print(\"\\nüèÜ MEILLEUR ALGORITHME PAR ENVIRONNEMENT :\")\n",
    "for env in environments:\n",
    "    env_results = results_df[results_df['Environment'] == env]\n",
    "    if not env_results.empty:\n",
    "        best = env_results.loc[env_results['Mean Score'].idxmax()]\n",
    "        print(f\"   {env:12} ‚Üí {best['Algorithm']:20} (Score: {best['Mean Score']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations des performances\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Heatmap des performances\n",
    "ax = axes[0, 0]\n",
    "sns.heatmap(pivot_scores, annot=True, fmt=\".3f\", cmap=\"viridis\", \n",
    "            cbar_kws={'label': 'Score moyen'}, ax=ax)\n",
    "ax.set_title(\"Heatmap des Performances\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Environnement\")\n",
    "ax.set_ylabel(\"Algorithme\")\n",
    "\n",
    "# 2. Temps d'entra√Ænement par algorithme\n",
    "ax = axes[0, 1]\n",
    "time_by_algo = results_df.groupby('Algorithm')['Training Time'].mean().sort_values()\n",
    "time_by_algo.plot(kind='barh', ax=ax, color='coral')\n",
    "ax.set_xlabel(\"Temps moyen (secondes)\")\n",
    "ax.set_title(\"Temps d'Entra√Ænement par Algorithme\", fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Performance par famille d'algorithmes\n",
    "ax = axes[1, 0]\n",
    "# Mapper les algorithmes aux familles\n",
    "algo_families = {\n",
    "    'policy_iteration': 'Dynamic Programming',\n",
    "    'value_iteration': 'Dynamic Programming',\n",
    "    'mc_es': 'Monte Carlo',\n",
    "    'mc_on_policy': 'Monte Carlo',\n",
    "    'mc_off_policy': 'Monte Carlo',\n",
    "    'sarsa': 'Temporal Difference',\n",
    "    'q_learning': 'Temporal Difference',\n",
    "    'expected_sarsa': 'Temporal Difference',\n",
    "    'dyna_q': 'Planning'\n",
    "}\n",
    "results_df['Family'] = results_df['Algorithm'].map(algo_families)\n",
    "family_perf = results_df.groupby('Family')['Mean Score'].mean().sort_values(ascending=False)\n",
    "family_perf.plot(kind='bar', ax=ax, color='skyblue')\n",
    "ax.set_ylabel(\"Score moyen\")\n",
    "ax.set_title(\"Performance Moyenne par Famille d'Algorithmes\", fontsize=14, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Variance des performances\n",
    "ax = axes[1, 1]\n",
    "for env in environments:\n",
    "    env_data = results_df[results_df['Environment'] == env]\n",
    "    if not env_data.empty:\n",
    "        ax.scatter(env_data['Mean Score'], env_data['Std Score'], \n",
    "                  label=env, s=100, alpha=0.7)\n",
    "ax.set_xlabel(\"Score moyen\")\n",
    "ax.set_ylabel(\"√âcart-type\")\n",
    "ax.set_title(\"Stabilit√© des Algorithmes (Score vs Variance)\", fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. √âtude des Hyperparam√®tres <a id=\"hyperparametres\"></a>\n",
    "\n",
    "### 5.1 Impact du Learning Rate (Œ±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtude de l'impact du learning rate sur Q-Learning\n",
    "print(\"üî¨ √âtude de l'impact du learning rate (Œ±) sur Q-Learning...\\n\")\n",
    "\n",
    "alpha_values = [0.01, 0.05, 0.1, 0.2, 0.5, 0.9]\n",
    "alpha_results = []\n",
    "\n",
    "# Test sur GridWorld\n",
    "for alpha in alpha_values:\n",
    "    print(f\"Testing Œ± = {alpha:4.2f} : \", end='', flush=True)\n",
    "    \n",
    "    params = runner.default_params.copy()\n",
    "    params['alpha'] = alpha\n",
    "    params['num_episodes'] = 500  # Moins d'√©pisodes pour acc√©l√©rer\n",
    "    \n",
    "    result = runner.run_experiment('gridworld', 'q_learning', params)\n",
    "    alpha_results.append({\n",
    "        'Alpha': alpha,\n",
    "        'Mean Score': result['mean_score'],\n",
    "        'Std Score': result['std_score'],\n",
    "        'Time': result['training_time']\n",
    "    })\n",
    "    \n",
    "    print(f\"Score: {result['mean_score']:.3f}\")\n",
    "\n",
    "alpha_df = pd.DataFrame(alpha_results)\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Performance vs Alpha\n",
    "ax1.plot(alpha_df['Alpha'], alpha_df['Mean Score'], 'o-', linewidth=2, markersize=8)\n",
    "ax1.fill_between(alpha_df['Alpha'], \n",
    "                 alpha_df['Mean Score'] - alpha_df['Std Score'],\n",
    "                 alpha_df['Mean Score'] + alpha_df['Std Score'],\n",
    "                 alpha=0.3)\n",
    "ax1.set_xlabel('Learning Rate (Œ±)')\n",
    "ax1.set_ylabel('Score moyen')\n",
    "ax1.set_title('Impact du Learning Rate sur la Performance')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Stabilit√© vs Alpha\n",
    "ax2.plot(alpha_df['Alpha'], alpha_df['Std Score'], 'o-', color='red', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Learning Rate (Œ±)')\n",
    "ax2.set_ylabel('√âcart-type du score')\n",
    "ax2.set_title('Impact du Learning Rate sur la Stabilit√©')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse\n",
    "best_alpha = alpha_df.loc[alpha_df['Mean Score'].idxmax()]\n",
    "print(f\"\\nüìä ANALYSE :\")\n",
    "print(f\"   ‚Ä¢ Meilleur Œ± pour la performance : {best_alpha['Alpha']} (Score: {best_alpha['Mean Score']:.3f})\")\n",
    "print(f\"   ‚Ä¢ Œ± trop petit (< 0.05) : apprentissage trop lent\")\n",
    "print(f\"   ‚Ä¢ Œ± trop grand (> 0.5) : instabilit√© et convergence d√©grad√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Impact de l'Exploration (Œµ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtude de diff√©rentes strat√©gies d'exploration\n",
    "print(\"üî¨ √âtude de l'impact de la strat√©gie d'exploration...\\n\")\n",
    "\n",
    "exploration_strategies = [\n",
    "    {'name': 'Forte d√©croissance', 'epsilon_start': 1.0, 'epsilon_decay': 0.99},\n",
    "    {'name': 'D√©croissance moyenne', 'epsilon_start': 1.0, 'epsilon_decay': 0.995},\n",
    "    {'name': 'Faible d√©croissance', 'epsilon_start': 1.0, 'epsilon_decay': 0.999},\n",
    "    {'name': 'Exploration constante', 'epsilon_start': 0.1, 'epsilon_decay': 1.0},\n",
    "]\n",
    "\n",
    "exploration_results = []\n",
    "\n",
    "for strategy in exploration_strategies:\n",
    "    print(f\"Testing {strategy['name']:20} : \", end='', flush=True)\n",
    "    \n",
    "    params = runner.default_params.copy()\n",
    "    params['epsilon_start'] = strategy['epsilon_start']\n",
    "    params['epsilon_decay'] = strategy['epsilon_decay']\n",
    "    params['num_episodes'] = 500\n",
    "    \n",
    "    result = runner.run_experiment('gridworld', 'sarsa', params)\n",
    "    exploration_results.append({\n",
    "        'Strategy': strategy['name'],\n",
    "        'Mean Score': result['mean_score'],\n",
    "        'Std Score': result['std_score']\n",
    "    })\n",
    "    \n",
    "    print(f\"Score: {result['mean_score']:.3f}\")\n",
    "\n",
    "# Visualisation\n",
    "exploration_df = pd.DataFrame(exploration_results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = range(len(exploration_df))\n",
    "plt.bar(x, exploration_df['Mean Score'], yerr=exploration_df['Std Score'], \n",
    "        capsize=10, color='lightgreen', edgecolor='darkgreen', linewidth=2)\n",
    "plt.xticks(x, exploration_df['Strategy'], rotation=45)\n",
    "plt.ylabel('Score moyen')\n",
    "plt.title('Impact de la Strat√©gie d\\'Exploration sur SARSA')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° CONCLUSION : Une d√©croissance moyenne de l'exploration offre le meilleur compromis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Impact de la Planification (Dyna-Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âtude du nombre d'√©tapes de planification dans Dyna-Q\n",
    "print(\"üî¨ √âtude de l'impact du nombre d'√©tapes de planification...\\n\")\n",
    "\n",
    "planning_steps = [0, 5, 10, 25, 50, 100]\n",
    "planning_results = []\n",
    "\n",
    "for n_steps in planning_steps:\n",
    "    print(f\"Testing {n_steps:3d} planning steps : \", end='', flush=True)\n",
    "    \n",
    "    params = runner.default_params.copy()\n",
    "    params['n_planning_steps'] = n_steps\n",
    "    params['num_episodes'] = 200  # Moins d'√©pisodes car Dyna-Q converge vite\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = runner.run_experiment('gridworld', 'dyna_q', params)\n",
    "    \n",
    "    planning_results.append({\n",
    "        'Planning Steps': n_steps,\n",
    "        'Mean Score': result['mean_score'],\n",
    "        'Training Time': result['training_time'],\n",
    "        'Efficiency': result['mean_score'] / result['training_time']  # Score par seconde\n",
    "    })\n",
    "    \n",
    "    print(f\"Score: {result['mean_score']:.3f}, Time: {result['training_time']:.2f}s\")\n",
    "\n",
    "planning_df = pd.DataFrame(planning_results)\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Performance vs Planning Steps\n",
    "ax1.plot(planning_df['Planning Steps'], planning_df['Mean Score'], \n",
    "         'o-', linewidth=2, markersize=8, color='blue')\n",
    "ax1.set_xlabel('Nombre d\\'√©tapes de planification')\n",
    "ax1.set_ylabel('Score moyen')\n",
    "ax1.set_title('Performance vs Planification')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Temps vs Planning Steps\n",
    "ax2.plot(planning_df['Planning Steps'], planning_df['Training Time'], \n",
    "         'o-', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_xlabel('Nombre d\\'√©tapes de planification')\n",
    "ax2.set_ylabel('Temps d\\'entra√Ænement (s)')\n",
    "ax2.set_title('Co√ªt Computationnel vs Planification')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficacit√©\n",
    "ax3.plot(planning_df['Planning Steps'], planning_df['Efficiency'], \n",
    "         'o-', linewidth=2, markersize=8, color='green')\n",
    "ax3.set_xlabel('Nombre d\\'√©tapes de planification')\n",
    "ax3.set_ylabel('Efficacit√© (score/seconde)')\n",
    "ax3.set_title('Efficacit√© vs Planification')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyse\n",
    "best_efficiency = planning_df.loc[planning_df['Efficiency'].idxmax()]\n",
    "print(f\"\\nüìä ANALYSE :\")\n",
    "print(f\"   ‚Ä¢ Meilleure efficacit√© : {best_efficiency['Planning Steps']} √©tapes\")\n",
    "print(f\"   ‚Ä¢ Au-del√† de 50 √©tapes, peu de gain en performance\")\n",
    "print(f\"   ‚Ä¢ Le co√ªt computationnel augmente lin√©airement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse Comparative <a id=\"analyse\"></a>\n",
    "\n",
    "### 6.1 Analyse par environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse d√©taill√©e pour chaque environnement\n",
    "for env_name in environments:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéØ ANALYSE D√âTAILL√âE : {env_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    env_data = results_df[results_df['Environment'] == env_name]\n",
    "    \n",
    "    if len(env_data) == 0:\n",
    "        print(\"‚ùå Aucune donn√©e disponible\")\n",
    "        continue\n",
    "    \n",
    "    # Top 3 algorithmes\n",
    "    top3 = env_data.nlargest(3, 'Mean Score')\n",
    "    print(\"\\nüèÜ TOP 3 ALGORITHMES :\")\n",
    "    for i, (_, row) in enumerate(top3.iterrows(), 1):\n",
    "        print(f\"   {i}. {row['Algorithm']:20} | Score: {row['Mean Score']:6.3f} ¬± {row['Std Score']:.3f} | \"\n",
    "              f\"Temps: {row['Training Time']:5.2f}s\")\n",
    "    \n",
    "    # Analyse par famille\n",
    "    family_analysis = env_data.groupby('Family')['Mean Score'].agg(['mean', 'std', 'count'])\n",
    "    print(\"\\nüìä PERFORMANCE PAR FAMILLE :\")\n",
    "    for family, stats in family_analysis.iterrows():\n",
    "        print(f\"   ‚Ä¢ {family:20} : {stats['mean']:6.3f} (n={stats['count']})\")\n",
    "    \n",
    "    # Recommandation\n",
    "    best = env_data.loc[env_data['Mean Score'].idxmax()]\n",
    "    print(f\"\\n‚úÖ RECOMMANDATION : {best['Algorithm']}\")\n",
    "    \n",
    "    # Justification sp√©cifique\n",
    "    if env_name == 'lineworld':\n",
    "        print(\"   Justification : Environnement simple et d√©terministe, tous les algorithmes convergent\")\n",
    "    elif env_name == 'gridworld':\n",
    "        print(\"   Justification : Espace d'√©tats mod√©r√©, b√©n√©ficie de la planification\")\n",
    "    elif env_name == 'rps':\n",
    "        print(\"   Justification : Environnement stochastique avec strat√©gie cach√©e\")\n",
    "    elif env_name == 'montyhall1':\n",
    "        print(\"   Justification : Probl√®me de d√©cision s√©quentielle avec information partielle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Synth√®se des forces et faiblesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau des forces et faiblesses\n",
    "strengths_weaknesses = pd.DataFrame([\n",
    "    {\n",
    "        'Algorithme': 'Policy Iteration',\n",
    "        'Forces': 'Convergence garantie, Optimal',\n",
    "        'Faiblesses': 'N√©cessite le mod√®le, Co√ªteux en m√©moire',\n",
    "        'Cas d\\'usage': 'Petits MDPs d√©terministes'\n",
    "    },\n",
    "    {\n",
    "        'Algorithme': 'Value Iteration',\n",
    "        'Forces': 'Plus simple que PI, Optimal',\n",
    "        'Faiblesses': 'N√©cessite le mod√®le, Convergence plus lente',\n",
    "        'Cas d\\'usage': 'MDPs avec horizon long'\n",
    "    },\n",
    "    {\n",
    "        'Algorithme': 'Monte Carlo ES',\n",
    "        'Forces': 'Sans mod√®le, Exploration garantie',\n",
    "        'Faiblesses': 'N√©cessite √©tats de d√©part vari√©s',\n",
    "        'Cas d\\'usage': 'Simulation possible partout'\n",
    "    },\n",
    "    {\n",
    "        'Algorithme': 'SARSA',\n",
    "        'Forces': 'On-policy (plus s√ªr), TD bootstrapping',\n",
    "        'Faiblesses': 'Convergence plus lente que Q-Learning',\n",
    "        'Cas d\\'usage': 'Apprentissage en ligne s√©curis√©'\n",
    "    },\n",
    "    {\n",
    "        'Algorithme': 'Q-Learning',\n",
    "        'Forces': 'Off-policy, Convergence vers Q*',\n",
    "        'Faiblesses': 'Peut sur-estimer les valeurs',\n",
    "        'Cas d\\'usage': 'Standard pour la plupart des cas'\n",
    "    },\n",
    "    {\n",
    "        'Algorithme': 'Dyna-Q',\n",
    "        'Forces': 'Combine exp√©rience et planification',\n",
    "        'Faiblesses': 'Co√ªt computationnel √©lev√©',\n",
    "        'Cas d\\'usage': 'Quand on peut apprendre un mod√®le'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"üìã TABLEAU DES FORCES ET FAIBLESSES\\n\")\n",
    "for _, row in strengths_weaknesses.iterrows():\n",
    "    print(f\"ü§ñ {row['Algorithme']}\")\n",
    "    print(f\"   ‚úÖ Forces : {row['Forces']}\")\n",
    "    print(f\"   ‚ùå Faiblesses : {row['Faiblesses']}\")\n",
    "    print(f\"   üéØ Cas d'usage : {row['Cas d\\'usage']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Environnements Secrets <a id=\"secrets\"></a>\n",
    "\n",
    "*Note: Cette section sera compl√©t√©e une fois les environnements secrets fournis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework pour tester les environnements secrets\n",
    "print(\"üîí PR√âPARATION POUR LES ENVIRONNEMENTS SECRETS\\n\")\n",
    "\n",
    "class SecretEnvironmentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.test_algorithms = [\n",
    "            ('q_learning', {'num_episodes': 5000, 'alpha': 0.1}),\n",
    "            ('sarsa', {'num_episodes': 5000, 'alpha': 0.1}),\n",
    "            ('dyna_q', {'num_episodes': 2000, 'n_planning_steps': 50}),\n",
    "            ('mc_on_policy', {'num_episodes': 5000})\n",
    "        ]\n",
    "    \n",
    "    def analyze_environment(self, env, env_name):\n",
    "        \"\"\"Analyse compl√®te d'un environnement inconnu.\"\"\"\n",
    "        print(f\"\\nüîç ANALYSE DE {env_name}\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # 1. Caract√©ristiques de base\n",
    "        print(f\"√âtats : {env.num_states()}\")\n",
    "        print(f\"Actions : {env.num_actions()}\")\n",
    "        \n",
    "        # 2. Test de stochasticit√©\n",
    "        print(\"\\nTest de stochasticit√©...\")\n",
    "        is_stochastic = self._test_stochasticity(env)\n",
    "        print(f\"Environnement {'stochastique' if is_stochastic else 'd√©terministe'}\")\n",
    "        \n",
    "        # 3. Test des algorithmes\n",
    "        print(\"\\nTest des algorithmes...\")\n",
    "        results = []\n",
    "        for algo_name, params in self.test_algorithms:\n",
    "            # Impl√©menter le test ici\n",
    "            pass\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _test_stochasticity(self, env, num_tests=10):\n",
    "        \"\"\"Teste si l'environnement est stochastique.\"\"\"\n",
    "        # Simplification : on suppose d√©terministe pour l'instant\n",
    "        return False\n",
    "\n",
    "analyzer = SecretEnvironmentAnalyzer()\n",
    "\n",
    "print(\"üìã M√©thodologie pr√©vue :\")\n",
    "print(\"1. Caract√©risation de l'environnement (taille, stochasticit√©)\")\n",
    "print(\"2. Test syst√©matique de tous les algorithmes\")\n",
    "print(\"3. Optimisation des hyperparam√®tres\")\n",
    "print(\"4. Analyse des strat√©gies apprises\")\n",
    "print(\"\\n‚è≥ En attente des environnements secrets...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions et Recommandations <a id=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä SYNTH√àSE FINALE DES R√âSULTATS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Classement global\n",
    "global_ranking = results_df.groupby('Algorithm')['Mean Score'].mean().sort_values(ascending=False)\n",
    "print(\"\\nüèÜ CLASSEMENT GLOBAL (score moyen sur tous les environnements) :\")\n",
    "for i, (algo, score) in enumerate(global_ranking.items(), 1):\n",
    "    print(f\"   {i:2d}. {algo:20} : {score:6.3f}\")\n",
    "\n",
    "# Recommandations finales\n",
    "print(\"\\nüéØ RECOMMANDATIONS FINALES :\")\n",
    "print(\"\\n1. CHOIX D'ALGORITHME :\")\n",
    "print(\"   ‚Ä¢ Environnement simple et petit ‚Üí Dynamic Programming (optimal)\")\n",
    "print(\"   ‚Ä¢ Environnement moyen, d√©terministe ‚Üí Dyna-Q (meilleur compromis)\")\n",
    "print(\"   ‚Ä¢ Environnement stochastique ‚Üí Q-Learning ou Expected SARSA\")\n",
    "print(\"   ‚Ä¢ Apprentissage en ligne s√©curis√© ‚Üí SARSA\")\n",
    "\n",
    "print(\"\\n2. HYPERPARAM√àTRES RECOMMAND√âS :\")\n",
    "print(\"   ‚Ä¢ Learning rate (Œ±) : 0.1 - 0.2\")\n",
    "print(\"   ‚Ä¢ Discount factor (Œ≥) : 0.9 - 1.0 selon l'horizon\")\n",
    "print(\"   ‚Ä¢ Exploration : Œµ‚ÇÄ=1.0, decay=0.995\")\n",
    "print(\"   ‚Ä¢ Planning (Dyna-Q) : 25-50 √©tapes\")\n",
    "\n",
    "print(\"\\n3. INSIGHTS CL√âS :\")\n",
    "print(\"   ‚Ä¢ Q-Learning offre le meilleur √©quilibre performance/simplicit√©\")\n",
    "print(\"   ‚Ä¢ Dyna-Q excelle quand on peut apprendre un bon mod√®le\")\n",
    "print(\"   ‚Ä¢ Les m√©thodes Monte Carlo souffrent sur les horizons longs\")\n",
    "print(\"   ‚Ä¢ Expected SARSA est plus stable que SARSA classique\")\n",
    "\n",
    "print(\"\\n4. PI√àGES √Ä √âVITER :\")\n",
    "print(\"   ‚Ä¢ Learning rate trop √©lev√© ‚Üí instabilit√©\")\n",
    "print(\"   ‚Ä¢ Exploration insuffisante ‚Üí convergence pr√©matur√©e\")\n",
    "print(\"   ‚Ä¢ Trop de planification ‚Üí co√ªt computationnel prohibitif\")\n",
    "print(\"   ‚Ä¢ Ignorer la nature de l'environnement ‚Üí mauvais choix d'algorithme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde des r√©sultats et politiques optimales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des r√©sultats\n",
    "output_dir = Path(\"results/final_report\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Sauvegarder le DataFrame des r√©sultats\n",
    "results_df.to_csv(output_dir / \"experimental_results.csv\", index=False)\n",
    "print(f\"‚úÖ R√©sultats sauvegard√©s : {output_dir / 'experimental_results.csv'}\")\n",
    "\n",
    "# 2. Sauvegarder les meilleures politiques\n",
    "best_policies = {}\n",
    "for env_name in environments:\n",
    "    env_results = [r for r in results if r['environment'] == env_name]\n",
    "    if env_results:\n",
    "        # Trouver le meilleur r√©sultat\n",
    "        best = max(env_results, key=lambda x: x['mean_score'])\n",
    "        best_policies[env_name] = {\n",
    "            'algorithm': best['algorithm'],\n",
    "            'policy': best['policy'],\n",
    "            'score': best['mean_score'],\n",
    "            'params': best['params']\n",
    "        }\n",
    "\n",
    "with open(output_dir / \"best_policies.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_policies, f)\n",
    "print(f\"‚úÖ Politiques optimales sauvegard√©es : {output_dir / 'best_policies.pkl'}\")\n",
    "\n",
    "# 3. G√©n√©rer un rapport markdown\n",
    "report = f\"\"\"# Rapport Final - Projet Reinforcement Learning\n",
    "\n",
    "## R√©sum√© Ex√©cutif\n",
    "\n",
    "- **Nombre d'exp√©riences** : {len(results)}\n",
    "- **Environnements test√©s** : {', '.join(environments)}\n",
    "- **Algorithmes √©valu√©s** : {len(algorithms)}\n",
    "\n",
    "## Meilleures Performances\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for env_name, policy_data in best_policies.items():\n",
    "    report += f\"### {env_name.capitalize()}\n",
    "- **Meilleur algorithme** : {policy_data['algorithm']}\n",
    "- **Score** : {policy_data['score']:.3f}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "report += \"\"\"## Recommandations\n",
    "\n",
    "1. **Q-Learning** pour la plupart des cas d'usage\n",
    "2. **Dyna-Q** quand la planification est possible\n",
    "3. **SARSA** pour l'apprentissage en ligne s√©curis√©\n",
    "\n",
    "## Configuration Optimale\n",
    "\n",
    "- Learning rate : 0.1-0.2\n",
    "- Exploration : Œµ-greedy avec decay=0.995\n",
    "- Planning steps : 25-50 pour Dyna-Q\n",
    "\"\"\"\n",
    "\n",
    "with open(output_dir / \"summary_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report)\n",
    "print(f\"‚úÖ Rapport markdown g√©n√©r√© : {output_dir / 'summary_report.md'}\")\n",
    "\n",
    "print(f\"\\nüìÅ Tous les fichiers sont disponibles dans : {output_dir}\")\n",
    "print(\"\\nüéâ PROJET TERMIN√â AVEC SUCC√àS !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexe : D√©monstrations Interactives\n",
    "\n",
    "### D√©monstration des politiques apprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration interactive d'une politique apprise\n",
    "def demonstrate_policy(env_name, policy, num_episodes=1):\n",
    "    \"\"\"Montre la politique en action.\"\"\"\n",
    "    env_class = runner.environments[env_name]\n",
    "    env = env_class()\n",
    "    visualizer = EnvironmentVisualizer(env)\n",
    "    \n",
    "    print(f\"\\nüéÆ D√©monstration de la politique sur {env_name}\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        steps = 0\n",
    "        \n",
    "        print(f\"\\n√âpisode {episode + 1} :\")\n",
    "        \n",
    "        while not env.is_game_over() and steps < 50:\n",
    "            state = env.state()\n",
    "            \n",
    "            # Choisir l'action selon la politique\n",
    "            if policy.ndim == 2:\n",
    "                action = np.argmax(policy[state])\n",
    "            else:\n",
    "                action = int(policy[state])\n",
    "            \n",
    "            # Afficher l'√©tat et l'action\n",
    "            print(f\"  Step {steps}: √âtat={state}, Action={action}\", end=\"\")\n",
    "            \n",
    "            # Ex√©cuter l'action\n",
    "            prev_score = env.score()\n",
    "            env.step(action)\n",
    "            reward = env.score() - prev_score\n",
    "            \n",
    "            print(f\", R√©compense={reward:+.1f}\")\n",
    "            steps += 1\n",
    "        \n",
    "        print(f\"\\n‚úÖ √âpisode termin√©! Score final: {env.score()}, Steps: {steps}\")\n",
    "\n",
    "# D√©monstration sur GridWorld avec la meilleure politique\n",
    "if 'gridworld' in best_policies:\n",
    "    demonstrate_policy('gridworld', best_policies['gridworld']['policy'])\n",
    "\n",
    "print(\"\\nüí° Pour une d√©monstration visuelle compl√®te, utilisez main_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes de D√©veloppement\n",
    "\n",
    "### Points d'am√©lioration identifi√©s\n",
    "\n",
    "1. **Optimisations possibles** :\n",
    "   - Vectorisation des calculs dans les algorithmes DP\n",
    "   - Parall√©lisation des runs pour les exp√©riences\n",
    "   - Cache des mod√®les pour les environnements d√©terministes\n",
    "\n",
    "2. **Extensions futures** :\n",
    "   - Impl√©mentation de Double Q-Learning\n",
    "   - Ajout de m√©triques suppl√©mentaires (sample efficiency)\n",
    "   - Visualisation en temps r√©el de l'apprentissage\n",
    "\n",
    "3. **Pour la soutenance** :\n",
    "   - Pr√©parer des d√©mos visuelles avec visualization.py\n",
    "   - Avoir les politiques pr√©-entra√Æn√©es charg√©es\n",
    "   - Pr√©parer des cas d'usage concrets\n",
    "\n",
    "---\n",
    "\n",
    "**Fin du rapport exp√©rimental**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}\
   